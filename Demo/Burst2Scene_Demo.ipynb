{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fa1cbf",
   "metadata": {},
   "source": [
    "# Burst2Scene AI - Project Demonstration\n",
    "This notebook demonstrates the inference pipeline of the Burst2Scene GAN-based model using a simplified sample of real burst images. It loads pretrained models and performs inference on a 10-frame burst sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ec594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057301b",
   "metadata": {},
   "source": [
    "## 📁 Check Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f55029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "required = [\n",
    "    Path('generator_last.pth'),\n",
    "    Path('discriminator_last.pth'),\n",
    "    Path('demo_burst/burst_00')\n",
    "]\n",
    "for path in required:\n",
    "    print(f\"{'✔️' if path.exists() else '❌'} {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cca7c",
   "metadata": {},
   "source": [
    "## 📦 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847daff",
   "metadata": {},
   "source": [
    "## 📂 Load Burst Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe8d81bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:25:39.753128Z",
     "start_time": "2025-06-02T08:25:38.425746Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class BurstDemoDataset:\n",
    "    def __init__(self, burst_path, max_frames=None):\n",
    "        self.burst_path = Path(burst_path)\n",
    "        self.frames = sorted(list(self.burst_path.glob('*.png')) + list(self.burst_path.glob('*.jpg')))\n",
    "        if not self.frames:\n",
    "            raise FileNotFoundError(f\"No image frames found in {burst_path}\")\n",
    "        if max_frames:\n",
    "            self.frames = self.frames[:max_frames]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def load_tensor(self):\n",
    "        tensors = []\n",
    "        for f in self.frames:\n",
    "            try:\n",
    "                img = Image.open(f).convert(\"RGB\")\n",
    "                tensors.append(self.transform(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {f.name}: {e}\")\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "# Update path to your actual burst folder\n",
    "dataset = BurstDemoDataset('demo_burst/burst_00')\n",
    "burst_tensor = dataset.load_tensor()\n",
    "print(f'Loaded burst shape: {burst_tensor.shape}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded burst shape: torch.Size([10, 3, 128, 128])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "4d19a052",
   "metadata": {},
   "source": "###Load Pretrained Generator & PatchGAN Discriminator"
  },
  {
   "cell_type": "code",
   "id": "667dbc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:25:53.493980Z",
     "start_time": "2025-06-02T08:25:53.381818Z"
    }
   },
   "source": [
    "from generator_colab import Generator\n",
    "from discriminator_PatchGAN import ConditionalDiscriminator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = ConditionalDiscriminator().to(device)\n",
    "\n",
    "generator.load_state_dict(torch.load('generator_last.pth', map_location=device))\n",
    "discriminator.load_state_dict(torch.load('discriminator_last.pth', map_location=device))\n",
    "\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "print('Models loaded successfully.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "6d8a04e1",
   "metadata": {},
   "source": [
    "## 🧪 Inference on Sample Burst"
   ]
  },
  {
   "cell_type": "code",
   "id": "187f0b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:26:50.303910Z",
     "start_time": "2025-06-02T08:26:50.128593Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def infer(model, burst):\n",
    "    burst = burst.unsqueeze(0).to(device)\n",
    "    output = model(burst)\n",
    "    return output.squeeze(0).cpu()\n",
    "\n",
    "result = infer(generator, burst_tensor)\n",
    "\n",
    "# Display first input frame and generated output\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(burst_tensor[0].permute(1, 2, 0).clamp(0, 1))\n",
    "axs[0].set_title('Input: First Burst Frame')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(result.permute(1, 2, 0).clamp(0, 1))\n",
    "axs[1].set_title('Generated Scene')\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 10, 3, 128, 128]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m     output = model(burst)\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output.squeeze(\u001B[32m0\u001B[39m).cpu()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m result = \u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mburst_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Display first input frame and generated output\u001B[39;00m\n\u001B[32m     10\u001B[39m fig, axs = plt.subplots(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m, figsize=(\u001B[32m10\u001B[39m, \u001B[32m5\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36minfer\u001B[39m\u001B[34m(model, burst)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;129m@torch\u001B[39m.no_grad()\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minfer\u001B[39m(model, burst):\n\u001B[32m      3\u001B[39m     burst = burst.unsqueeze(\u001B[32m0\u001B[39m).to(device)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m     output = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mburst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output.squeeze(\u001B[32m0\u001B[39m).cpu()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/generator_colab.py:72\u001B[39m, in \u001B[36mGenerator.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     x1 = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minput_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m     x2 = \u001B[38;5;28mself\u001B[39m.down1(x1)\n\u001B[32m     74\u001B[39m     x3 = \u001B[38;5;28mself\u001B[39m.down2(x2)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/generator_colab.py:23\u001B[39m, in \u001B[36mConvBlock.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Master's years/Demo_ACIT4630/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 10, 3, 128, 128]"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "922b4b0c",
   "metadata": {},
   "source": [
    "## 📸 Run Model Evaluation Preview (Inlined `preview.py`)"
   ]
  },
  {
   "cell_type": "code",
   "id": "805aead2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T08:29:44.274146Z",
     "start_time": "2025-06-02T08:29:43.570281Z"
    }
   },
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from validation_dataset_colab import BurstDataset\n",
    "from generator_colab import Generator\n",
    "from discriminator_PatchGAN import ConditionalDiscriminator  # Adjusted to correct PatchGAN\n",
    "import piq\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# --- Config ---\n",
    "checkpoint_path = \"generator_last.pth\"  # ← now points to root\n",
    "burst_dir = \"burst_validation_high_variation\"\n",
    "preview_name = \"preview_patchgan\"\n",
    "base_preview_dir = os.path.join(\"model_preview_dir\", preview_name)\n",
    "os.makedirs(base_preview_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load models ---\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "generator.eval()\n",
    "\n",
    "discriminator = ConditionalDiscriminator().to(device)\n",
    "discriminator.eval()\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = BurstDataset(bursts_dir=burst_dir, burst_size=10)\n",
    "\n",
    "unnorm = lambda x: (x + 1) / 2\n",
    "\n",
    "def annotate_image(image_path, text):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    width, height = image.size\n",
    "    draw.rectangle([(0, height - (bbox[3] - bbox[1]) - 10), (width, height)], fill=(0, 0, 0))\n",
    "    draw.text((10, height - (bbox[3] - bbox[1]) - 5), text, fill=\"white\", font=font)\n",
    "    image.save(image_path)\n",
    "\n",
    "# --- Inference loop ---\n",
    "for i in range(len(dataset)):\n",
    "    burst, target = dataset[i]\n",
    "    burst = burst.to(device)\n",
    "    target = target.to(device)\n",
    "    burst_input = burst.view(1, -1, burst.size(2), burst.size(3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = generator(burst_input)\n",
    "        confidence = torch.sigmoid(discriminator(burst_input, output)).mean().item()\n",
    "\n",
    "    output = unnorm(output.squeeze(0))\n",
    "    target = unnorm(target)\n",
    "    burst_frames = [unnorm(frame) for frame in burst]\n",
    "\n",
    "    # Save visual outputs\n",
    "    sample_dir = os.path.join(base_preview_dir, f\"sample_{i}\")\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(sample_dir, \"generated_output.png\")\n",
    "    save_image(output, output_path)\n",
    "    save_image(target, os.path.join(sample_dir, \"target_frame.png\"))\n",
    "    save_image(make_grid(burst_frames, nrow=5), os.path.join(sample_dir, \"burst_grid.png\"))\n",
    "\n",
    "    # Evaluate\n",
    "    output_u = output.unsqueeze(0)\n",
    "    target_u = target.unsqueeze(0)\n",
    "    psnr_score = piq.psnr(output_u, target_u, data_range=1.0).item()\n",
    "    ssim_score = piq.ssim(output_u, target_u, data_range=1.0).item()\n",
    "\n",
    "    label = f\"PSNR = {psnr_score:.2f}, SSIM = {ssim_score:.4f}, Conf = {confidence:.4f}\"\n",
    "    annotate_image(output_path, label)\n",
    "    with open(os.path.join(sample_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(label + \"\\n\")\n",
    "\n",
    "    print(f\"🖼 Sample {i}: {label}\")\n",
    "\n",
    "# Write full summary\n",
    "with open(os.path.join(base_preview_dir, \"info.txt\"), \"w\") as log_file:\n",
    "    for i in range(len(dataset)):\n",
    "        sample_metrics_path = os.path.join(base_preview_dir, f\"sample_{i}\", \"metrics.txt\")\n",
    "        if os.path.exists(sample_metrics_path):\n",
    "            with open(sample_metrics_path, \"r\") as f:\n",
    "                log_file.write(f\"Sample {i}: {f.readline().strip()}\\n\")\n",
    "\n",
    "print(f\"✅ Previews saved to: {base_preview_dir}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼 Sample 0: PSNR = 23.49, SSIM = 0.7337, Conf = 0.0664\n",
      "🖼 Sample 1: PSNR = 16.40, SSIM = 0.2263, Conf = 0.0312\n",
      "✅ Previews saved to: model_preview_dir/preview_patchgan\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
